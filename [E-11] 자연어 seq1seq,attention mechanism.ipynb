{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목표\n",
    "\n",
    "1. 뉴스 요약봇 만들기 \n",
    "\n",
    "* seq2seq 이해하기 \n",
    "* Attention Mechanism \n",
    "* LSTM 이해하기 \n",
    "* NLTK의 불용어(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잠깐 개념이해!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "= 두개의 RNN구조를 사용해서 입력문장에서 출력문장을 만드는 자연어 생성 모델이다. \n",
    "\n",
    "#사진추가 \n",
    "\n",
    "문장을 첫번째 RNN encoder에 입력하면 마지막 time step의 hidden state의 값을 컨텍스트 벡터로 사용한다. \n",
    "두번째 RNN decoder에 컨텍스트 벡터를 전달해서 한 단어씩 요약문장을 만든다.\n",
    "\n",
    "하지만 이때 생기는 문제가 있다. \n",
    "\n",
    "컨텍스트 벡터는 고정된 크기의 vector만 담을 수 있다. 만약 문장이 길거나 컨텍스트 벡터의 크기가 입력된 문장에 비해 작으면 모든 정보를 담을 수 없다. \n",
    "즉, 요약이나 번역이 원래 의도를 표현하지 못한다. \n",
    "\n",
    "Attention Mechanism을 적용해서 정보 손실을 막을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "우리는 seq2seq를 구현할때 RNN(인코더,디코더)이 아닌 LSTM으로 인코더/디코더를 만들기 때문에 개념을 이해하고 넘어가자\n",
    "\n",
    "RNN을 역전파 할때 관련 정보와 다른 정보 사이의 거리가 멀 경우 그라디언트 손실이 발생한다. 이때 이를 해결하기 위해서 LSTM 사용한다. \n",
    "\n",
    "LSTM이란?\n",
    "\n",
    "\n",
    "* 특징\n",
    " RNN의 HIDDEN STATE에 CELL STATE를 추가한다. \n",
    " CELL STATE는 gate를 사용해서 문장을 더하거나 뺄 수 있다. \n",
    " \n",
    " 사진 첨가해서 설명할것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "Attention Mechanism은 인코더의 모든 step의 hidden 값을 컨텍스트 벡터에 반영한다. 이때 동일한 비중으로 hidden 값이 전달되지 않는다. \n",
    "fully connected layer와 softmax를 사용해 전달될 hidden 값의 비중을 정한다. \n",
    "\n",
    "#사진 \n",
    "사진으로 설명 보충 금요일날 하기 (유튜브에서 사진)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 흐름 정리 \n",
    "기존: seq2seq(기본모델)+2가지 RNN 사용\n",
    "\n",
    "오늘 사용할 것\n",
    "= 기존 seq2seq(기본모델)-> seq2seq+LSTM 사용(cell state)+ attention Mechanism 모듈\n",
    "\n",
    "* seq2seq를 구동시키면 디코더는 시작 토큰을 입력받아 예측을 시작합니다.\n",
    "* 디코더에서 시퀸스 앞,뒤에 시작 토큰 SOS,예측 토큰 EOS를 붙인다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
